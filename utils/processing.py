from langgraph.graph.state import CompiledStateGraph
from PIL import Image
import io
from pathlib import Path
from langchain_core.messages import HumanMessage
from utils.state import AgentState
from langchain_core.runnables.graph import MermaidDrawMethod
import time
from typing import List

async def get_topics(folder_path: str) -> List[str]:
    """
    Returns the vector stores topics.
    This is a simple implementation that uses the folder names as topics

    Parameters:
        folder_path (str): the path to the folder containing the vector stores
    
    Returns:
        List[str]: a list of string containing the topics
    """
    store_dir = Path(folder_path)

    if not store_dir.exists():
        raise FileNotFoundError(f"{store_dir} is not a existing directory")
    
    topics=["measurement_unit_conversion"]
    for dir in store_dir.iterdir():
        topics.append(dir.name)

    return topics

async def save_to_png(agent: CompiledStateGraph, file_name: str) -> None:
    """
    Save a graphic representation of an agent/graph to the file system as png image

    Parameters:
        agent (CompiledStateGraph): the agent/graph compiled
        file_name (str): the path and name of the generated image **It must contain the .png extension**
    """
    img_data = agent.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)
    img = Image.open(io.BytesIO(img_data))
    img.save(file_name)

async def stream_response(agent: CompiledStateGraph[AgentState], user_query: str, chat_history: str,  verbosity: int = 0) -> str:
    """
    Print on the console, the steps/nodes invoked by the agent to answer the user's query

    Parameters:
        agent (CompiledStateGraph[AgentState]): the agent/compiled graph
        user_query (str): the user's query/question that the agent try to answer
        chat_history (str): the current chat history between the user and the agent
        verbosity (int): a non negative integer used to control the granuality of the informations showed on the console. [0 -> Only the steps, 1 -> Steps and Messages, 2 -> Steps, Messages, and State]
    
    Returns:
        str: The final answer generated by the agent
    """
    last_msg = None
    prefix = "\n" if verbosity > 0 else ""
    start = time.time()

    # Using .astream() [and async for loop] because the tools loaded from custom MCP server (type: StructuredTool) can only be used asynchronously.
    # At the time of writing The synchronous methods are not implemented yet
    async for event in agent.astream(AgentState.create(messages=[HumanMessage(user_query)], question=user_query, history=chat_history)):
        # An event is generated every time a node is executed 
        # An event is the agent's state after each node execution
        end = time.time()
        for key, value in event.items():
            print(f"{prefix}STEP: {key} ({(end - start):.2f}s)", flush=True)
            last_msg =  value["messages"][-1]
            if verbosity > 0:
                last_msg.pretty_print()
                if verbosity > 1:
                    if last_msg.type != "tool":
                        print(f"\n{'-'*36} STATE {'-'*37}")
                        print(f"Question: {value["question"]}")
                        print(f"Original Question: {value["original_question"]}")
                        context = (value["context"][:100]) if value["context"] else ""
                        print(f"Context: {context}")
                        print(f"Reranking score: {value["reranking_score"]}")
                        chunks = (str(value["chunks"])[:100]) if value["chunks"] else value["chunks"]
                        print(f"Chunks: {chunks}")
                        print(f"Chat History:\n{chat_history}")
                        print(f"{'-'*80}")
        start = end

    return last_msg.content